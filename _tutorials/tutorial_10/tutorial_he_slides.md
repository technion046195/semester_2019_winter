---
number: 10
title: "CNN" 
---
<section markdown="1">
## Convolutional Neural Networks (CNN) - רשתות קונבולוציה

אחת מהתכונות של רשת Fully Connected (FC) MLP היא חוסר רגישות לסדר בכניסות לרשת. תכונה זאת נרכשת משום שכל היחידות בכל שכבה מחוברות לכל היחידות בשכבה העוקבת. במקרים רבים תכונה זאת נדרשת אך עולה במספר רב של פרמטרים.

לכן במקרים בהם לכניסות לרשת (data) יש מבנה או תלות מרחבית, כלומר יש משמעות לסדר של הכניסות, נרצה לנצל את ההיתרון המרחבי של הכניסות בזמן קביעת ארכיטקטורת הרשת. דוגמא לסוג כזה של מידע הן תמונות.

רשת קונבולציה היא רשת feed-forward בעלת ארכיטקטורה אשר מנצלת את התלות המרחבית בכניסות לכל שכבה.

</section><section markdown="1">

## 1D Convolutional Layer
קונבולוציה חד-ממדית מבצעת פעולת קרוס קורלציה בין שני ווקטורים, וקטור הכניסות $$\boldsymbol{x}$$ ווקטור המשקולות $$\boldsymbol{w}$$ באורך $$K$$: 

$$
\boldsymbol{y}\left[n\right]=\sum_{m=0}^{K-1} \boldsymbol{x}\left[n+m\right]\boldsymbol{w}\left[m\right]
$$

וקטור המשקולות של שכבת הקונבולציה $$\boldsymbol{w}$$ נקרא **גרעין הקונבולוציה (convolution kernel)** 
 
**הערה:**
הפעולה היא **לא** פעולת קונבולציה כפי שאתם מכירים: $$\boldsymbol{y}\left[n\right]=\sum_{m=0}^{K-1} \boldsymbol{x}\left[n-m\right]\boldsymbol{w}\left[m\right]$$

ובאופן גרפי:

![png](figs/conv_layer.png)

נשים לב להבדלים מרשת FC:

1. היציאות מחוברות לחלק מהכניסות, כאשר הכניסות קרובות אחת לשניה
2. כל היציאות מופקות מאותן משקולות

</section>

ההנחות שמובילות לארכיטקטורה זאת:
- הקשר בין כניסות קרובות הוא יותר חזק ומשמעותי מאשר כניסות רחוקות.
-  אזורים מקומיים במידע חולקים את אותם מאפיינים מקומיים משותפים, כך שלאזורים שונים יש מאפיינים שונים.

שכבת קונבולוציה מורידה באופן דרסטי את מס' במשקולות לעומת שכבת FC. בשכבת FC קיימות $$N_\text{inputs}\times N_\text{outputs}$$ משקולות בעוד שלשכבת קונבולציה יש $$K$$ משקולות.

דרך נוספת להצגת שכבת קונבולוציה:

![Alt Text](figs/conv_layer.gif)

כאשר:  $$h\left(\boldsymbol{z}\right)=\boldsymbol{w}^T\boldsymbol{z}=w_1z_1+w_2z_2+w_3z_5+b$$, ו- $$b$$ היינו איבר ההסט.


## אקטיבציה לא לינארית 
באותו אופן עבור שכבות FC, בשכבת קונבולציה נעשה שימוש בפונקציות אקטיבציה לא לינאריות לאחר פעולת הקונבולציה שהיינה פעולה לינארית.
 גם כאן השימוש הכי נפוץ היינו בפונקציית ReLU: $$\varphi\left(x\right)=\max\left(x,0\right)$$, אך ניתן להשתמש בכל פונקציה אחרת.
 בפועל מפרידים בין פעולת הקונבולוציה לבין פעולת האקטיבציה והארכיטקטורה נראת כך:
 
 ![png](figs/conv_layer_nonlinear.png)


##  קלט רב-ערוצי 
במקרים רבי נרצה ששכבת הקונבולציה תקבל קלט רב ממדי, לדוגמא, תמונה בעלת שלושה ערוצי צבע או קלט שמע ממספר ערוצי הקלטה. מבנה זה מאפשר לאזור מרחבי בקלט להכיל אינפורמציה ממספר ערוצי כניסה.

על כן הניורון $$h$$ הוא פונקציה של כל ערוצי הקלט:

![png](figs/conv_layer_multi_channel.gif)

הפונקציה $$h$$ היינה קומבינציה לינארית של כל ערוצי הקלט, לרוב מסיפים איבר הסט $$b$$ ופונקציית אקטיבציה.


## פלט רב-ערוציי 
נקודה נוספת, לרוב על איזור מרחבי מסויים נרצה להפעיל יותר מגרעין קונבולציה אחד $$h$$, המטרה להשיג מספר גדול יותר של מאפיינים על אותו אזור שמתורגם לפלט רב-ממדי:

![Alt Text](figs/conv_layer_multi_channel2.gif)

בארכיטקטורה הזאת אין שיתוף בין משקולות הגרעינים לערוצים הפלט השונים, כלומר כל גרעין קונבולציה הוא בעל סט משקולות יחודי הפועל על כל הערוצים על מנת להוציא פלט יחיד. 
מספר הפרמטרים בשכבת כזאת היינו:  $$\underbrace{C_\text{in}\times C_\text{out}\times K}_\text{the weights}+\underbrace{C_\text{out}}_\text{the bias}$$

כאשר:
- $$C_\text{in}$$ - מספר ערוצי קלט.
- $$C_\text{out}$$ - מספר ערוצי פלט.
- $$K$$ - גודל הגרעין.

**דגש:** שימו לב, יש להבדיל בין מס' הערוצים לממד הקלט/פלט. נגיד עבור ערוץ אחד אנחנו יכולים לקבל קלט רב ממדי (נגיד תמונה) ועבור פלט רב ערוצי, עבור כל ערוץ אנחנו יכולים לקבל מס' ממדים עבור כל ערוץ
## היפר-פרמטרים של שכבות קונבולוציה 
**גודל הגרעין** ו **מספר ערוצי הפלט** הינם היפר-פרמטרים שעל מתכנן הרשת לקבוע.

בנוסף יש תכונות נוספות על שכבת הקונבולציה שיש לקבוע שגם נחשבים כהיפר-פרמטרים ועליהם נעבור בחלקים הבאים:

### Padding - ריפוד
משום שפעולת קונבולציה היינה מרחבית, בקצוות הקלט ישנה בעיה שאין ערכים חוקיים שניתן לבצע עליהם פעולות, לכן נהוג לרפד את שולי הקלט (באפסים או שכפול של אותו ערך בקצה)

![Alt Text](figs/conv_layer_padding.gif)

### Stride - צעידות
ניתן להניח שלרוב הקשר המרחבי נשמר באזורים קרובים, לכן על מנת להקטין בחישוביות ניתן לדלג על הפלט ולהפעיל את פעולת הקונבולציה באופן יותר דליל. בפשטות: מדלגית על היציאות בגודל הצעד.
לרוב גודל הצעד מסומן ב $$s$$, בדוגמא הבאה גודל הצעד היינו $$s=2$$ .

![Alt Text](figs/conv_layer_stride.gif)

### Dilation - התרחבות
שוב על מנת להקטין בחישובית, אפשר לפעול על אזורים יותר גדולים תוך הנחה שערכים קרובים גיאוגרפית הם בעלי ערך זהה, על כן נרחיב את פעולת הקונבולציה תוך השמטה של ערכים קרובים.
לרוב נסמן את ההתרחבות ב $$d$$ בדוגמא הבאה $$d=2$$.

![Alt Text](figs/conv_layer_dilation.gif)

**דגש:** צעידות מצמצמות את הפלט, התרחבות גם כן מצמצמת את הפלט אך על חשבון התרחבות על הקלט. 

## Max Pooling
לרוב ב CNN נעשה שימוש בשכבת נוספות על מנת לצמצם את הגודל המרחבי של הקלט. שכבה כזאת לדוגמא היינה שכבת Max Pooling, שכבה זאת לוקחת את המקסימום מבין ערכי הכניסה. המוטיבציה לפעולה זאת היינה שהערכים הגבוהים מייצגים מאפיינים בעלי יותר אינפורמציה על כן נרצה לשמר את הערכים אלה על חשבון המאפייינים בעלי פחות אינפורציה. 

בדוגמא הבאה גודל ב Max Pooling היינו 2 וגודל הצעד (stride) גם כן 2:

![Alt Text](figs/max_pool.gif)

בשכבה זאת אין פרמטרים נלמדים, אך גודל הגרעין היינו היפר-פרמטר נוסף.

## 2D Convolutional Layer
עבור קלט דו-ממדי (תמונות), הקלט מסודר כמטריצה. ופעולת הקונבולוציה (קרוס-קורלציה כפי שאתם מכירים) נראה כך:

כאשר השכבה הכחולה היא הקלט והשכבה הירוקה היא הפלט

<table style="width:100%; table-layout:fixed;">
  <tr>
    <td><center>kernel size=3<br>padding=0<br>stride=1<br>dilation=1</center></td>
    <td><center>kernel size=4<br>padding=2<br>stride=1<br>dilation=1</center></td>
    <td><center>kernel size=3<br>padding=1<br>stride=1<br>dilation=1<br>(Half padding)</center></td>
    <td><center>kernel size=3<br>padding=2<br>stride=1<br>dilation=1<br>(Full padding)</center></td>
  </tr>
  <tr>
    <td><img width="150px" src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides.gif?raw=true"></td>
    <td><img width="150px" src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/arbitrary_padding_no_strides.gif?raw=true"></td>
    <td><img width="150px" src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/same_padding_no_strides.gif?raw=true"></td>
    <td><img width="150px" src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/full_padding_no_strides.gif?raw=true"></td>
  </tr>
  <tr>
    <td><center>kernel size=3<br>padding=0<br>stride=2<br>dilation=1</center></td>
    <td><center>kernel size=3<br>padding=1<br>stride=2<br>dilation=1</center></td>
    <td><center>kernel size=3<br>padding=1<br>stride=2<br>dilation=1</center></td>
    <td><center>kernel size=3<br>padding=0<br>stride=1<br>dilation=2</center></td>
  </tr>
  <tr>
    <td><img width="150px"src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_strides.gif?raw=true"></td>
    <td><img width="150px" src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides.gif?raw=true"></td>
    <td><img width="150px" src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides_odd.gif?raw=true"></td>
    <td><img width="150px" src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/dilation.gif?raw=true"></td>
  </tr>
</table>

* \[1\] Vincent Dumoulin, Francesco Visin - [A guide to convolution arithmetic
  for deep learning](https://arxiv.org/abs/1603.07285)
  ([BibTeX](https://gist.github.com/fvisin/165ca9935392fa9600a6c94664a01214))
  

## שאלה 1: 
נתונה רשת קונבולוציה קטנה, הממירה תמונה בגודל 13×13 לווקטור מוצא בגודל 4×1. הרשת מורכבת מהפעולות הבאות:
 משכבות קונבולוציה עם 3 פילטרים (גרעינים) בגודל 4x4, פונקציית אקטיבציה Relu, max pooling  ו Fully-connected (FC) בסוף. ברשת זאת אין שימוש בפרמטר חופשי -bias .
![png](Q1.png)
 	
 א. כמה משקולות יש בשכבת הקונבולוציה שעוברות למידה?
 
ב. כמה פעולות אקטיבציה (Relu) במעבר קדמי אחד (forward pass)?

ג. כמה משקולות יש בכל הרשת?

ד. האם רשת בקישוריות מלאה (FC) בעלת גודל שכבות זהה – כלומר ישנם 13×13  כניסות, שכבה נסתרת ראשונה בעלת 3×10×10 ניורונים וכן הלאה יכולה לייצג אותו מסווג שהרשת לעיל מייצגת?

ה. מה ההבדל העיקרי בין רשת CNN ל FC שיכולות לייצג את אותו מסווג?

## פתרון: 

א. סה"כ 48 משקולות. קיימים 3 פילטרים בגודל 4×4. ישנה משקולת בכל תא בפילטר לכן 3⋅4⋅4=48.

ב. מבוצעות 304 פעולות Relu. פעולת האקטיבציה מתבצעת לכל מאפיין בשכבת הקונבולוציה ועוד 4 בשכבת הFC. כלומר:
$$10 \cdot 10 \cdot 3 + 4 =304$$

ג. 348 משקולות. נפרט את החישוב, 48 לשכבת הקונבולוציה. לכל ה75 פיקסלים לאחר שכבת max pooling מחוברים לכל 4 הנוירונים אז סה"כ 4⋅75+48=348.

ד. כן, הרשתות שקולות, עם התאמה נכונה של המשקולות ברשת FC.

ה. מספר הפרמטרים הנלמדים. רשת FC בעלת מספר רב של פרמטרים. לצורך הדוגמא נחשב את במספר הפרמטרים רק עבור השקילות לשכבת הקונבולוציה,
13⋅13⋅3⋅10⋅10∼50k 
כמן כן נשים לב ל Bias-Variance ,
ברגע אנחנו מבינים שיש תלות מרחבית מקלט ומקטינים את מס' המשקולות אנחנו מעלים את ה bias כך שהפתרון יאולץ לפי המודל שאנחנו חושבים שהקלט בנוי. באופן ישיר ה variance קטן משום שמספר המשקולות קטן.

## שאלה 2 
עלתה דרישה לתכנן פילטר בשכבת קונבולוציה בגודל 7×7, אך ורק על יד פילטרים בגודל 3×3.

א. הציעו מימוש?

ב. רשמו באופן מטריצי את פעולת הקונבולוציה:

## פתרון 
א. נוכל לממש על ידי העברת פילטר 3×3 שלוש פעמים ושילוב של קומבינציה לינארית על פאץ של 7×7. 
שלב ראשון:
![png](Q2a.png)

שלב שני:
![png](Q2b.png)

שלב שלישי:
![png](Q2c.png)

**הערה:** האזור בקלט שמשפיע על יציאה בודדת נקרא ה receptive field

ב. נרשום את הפילטרים ככפל מטריצות, עבור הפילטר הראשון:

$$
w^1 = 
\begin{pmatrix}
w_1 & w_2 & w_3 & 0 & \cdots & 0 & w_4 & w_5 & w_6 & 0 & \cdots \\
0 & w_1 & w_2 & w_3 & 0 & \cdots & 0 & w_4 & w_5 & w_6 &  \cdots \\
\vdots & \vdots \vdots  & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots 
\end{pmatrix}
\in \mathbb{R}^{d_1\times d_0 = 25 \times 49}
$$

ובאופן זהה עבור פילטר מס' שתיים ושלוש.

$$
w^2 \in \mathbb{R}^{d_2\times d_1 = 9 \times 25} 
$$

$$
w^3 \in \mathbb{R}^{d_3\times d_2 = 1 \times 9}
$$

נרשום את הקלט בצורה וקטורית: $$ x = (x_1, \cdots, x_{49})^T$$

וסה"כ נקבל:
$$
y=w^3 w^2 w^1 x
$$

## סכמות אתחול
### מוטיבציה: 

- ראינו בתרגול 8 דוגמה שהמחישה כי בחירה לא טובה של האתחול יכולה להוביל לכך שפונקציית האקטיבציה תהיה באזור של רוויה ולכן העדכון של המשקולות באלגוריתם GD יהיה מאוד איטי ולא אפקטיבי.
- אם המשקולות של הרשת מתחילות בערכים קטנים מדי אז האות דועך ככל שהוא מתקדם לאורך הרשת ולא מחלחל בצורה טובה לשכבות המתקדמות.
- אם המשקולות של הרשת מתחילות בערכים גדולים מדי אז האות גדל בין שכבה לשכבה מה שיכול לגרום לבעיות באימון (חריגה מתחום ייצוג, vanishing\exploding gradients).
![png](init_1.png)
![png](init_2.png)

- ניתן לראות באיורים כי האות לא מפעפע בצורה טובה דרך הרשת:
    - באיור הראשון אנו רואים כי ככל שמתקדמים לעומק הרשת האקטיבציות דועכות, כלומר, יש יותר אקטיבציות עם ערכים מאוד קרובים לאפס.
    - באיור השני אנו רואים את הבעיה של vanishing gradients שלמדנו בתרגול הקודם.
    
### המטרה: 
נרצה למצוא אתחול טוב של המשקולות אשר יאפשר לאות לפעפע דרך הרשת בצורה טובה.


## שאלה 3: 
בשאלה זו נרצה למצוא סכמת אתחול עבור משקולות הרשת. לשם כך, נתחיל מלבחון נוירון ליניארי בודד.

![png](Q3_1.png)

**הערה**: אם הממוצע של המוצא $$v$$ הוא 0 והשונות נשארת קבועה בין שכבות (לא דועכת או מתפוצצת) – אין התפוצצות או העלמות של האות לאורך הרשת. לכן, בסעיפים הבאים נתעניין בחישוב הממוצע והשונות של המוצא $$v$$.
 ספציפית: נרצה להראות כי $$Ev=0$$ ואז לחשב את השונות של המוצא על מנת לראות מה עלינו לדרוש על אתחול המשקולות על מנת להבטיח כי שונות המוצא תישאר קבועה לאורך הרשת.
 
**א.** נניח כי כל רכיבי המשקולות וכל רכיבי הכניסה הם משתנים אקראיים IID. בנוסף, נניח כי התפלגות המשקולות סימטרית סביב 0.

1. חשבו את התוחלת של המוצא v כתלות בתוחלות של המשקולות והכניסה. בפרט, הראו כי $$Ev=0$$.
    
2. הראו כי לכל זוג משתנים אקראיים בלתי תלויים $$w_i,x_i$$ מתקיים:
$$ Var(w_i x_i )=(Ew_i )^2 Var(x_i )+(Ex_i )^2 Var(w_i )+Var(x_i )Var(w_i ) \quad (*) $$

3. כעת, נניח כי לכל $$i$$: $$Ex_i=0$$ (בסעיף הבא נראה הצדקה להנחה זו).
השתמשו בנוסחה מהסעיף הקודם על מנת לבטא את השונות של $$v$$ באמצעות השונות של $$x_1,w_1$$.

4. כיצד ניתן לשמור על השונות של המוצא v זהה לשונות של כל אחת מרכיבי הכניסה $$x_i$$?


**ב.** כעת, נרצה להרחיב את הסכמה לנוירונים **לא ליניאריים**:

![png](Q3_2.png)

כלומר, כעת מתקיים כי $$x_i=\sigma(y_i ), \forall i=1, \dots,n$$
כאשר $$sigma (\cdot)\$$  (היא פונקציית האקטיבציה ו-
$$v=w_1 x_1+w_2 x_2+⋯+w_n x_n$$

אנו נניח כי ניורון זה הוא חלק מרשת עמוקה, כלומר, נניח כי $$y_1,\dots,y_n$$ הם המוצאים של ניורונים מהשכבה הקודמת ברשת.
**הערה:** בסעיף הבא נראה כי התוחלת של המוצא איננה 0 עבור אחת מהאקטיבציות שלמדנו בקורס. עם זאת, נראה כי הממוצע לפני האקטיבציה, כלומר, הממוצע של $$y_i$$,
 הוא עדיין 0 לכל i. לכן, אין לנו בעיה של התפוצצות הממוצע $$\leftarrow$$ אם השונות נשארת קבועה בין שכבות (לא דועכת או מתפוצצת) – אין התפוצצות או העלמות של האות לאורך הרשת.

1. לאילו מפונקציות האקטיבציה שלמדנו בקורס (sigmoid, tahn ו-ReLU) ההנחה $$Ex_i=0$$ שביצענו בסעיף הקודם היא עדיין הנחה "סבירה"?
תזכורת:
![png](Q3_3.png)

2. 	כעת לא נניח דבר על התוחלת של  $$x_i$$. כיצד ישתנה הביטוי של השונות של  $$v$$ מסעיף א.3?

3.	עבור פונקציית האקטיבציה ReLU, בטאו את השונות של $$v$$ באמצעות השונות של אחת הכניסות $$y_i$$. הניחו כי $$Var(y_1 )=Var(y_2 )=\cdots=Var(y_n)$$ 

הדרכה: היעזרו בתוצאות הסעיפים הקודמים.

## פתרון: 
**א.**

1.

    $$
    Ev=E[w_1 x_1+w_2 x_2+⋯+w_n x_n ]=nE[w_1 x_1 ]=nE[w_1 ]E[x_1 ]=0
    $$

2.
    נעזר בקשר $$Var(z)=Ez^2-(Ez)^2$$ שמתקיים עבור כל משתנה אקראי $$z$$ ובעובדה שנתון כי $$w_i,x_i$$  בלתי תלויים. נקבל באגף השמאלי של (*):
    
    $$
    (Ew_i )^2 Var(x_i )+(Ex_i )^2 Var(w_i )+Var(x_i )Var(w_i )
    $$
    
    $$
    =(Ew_i )^2 (Ex_i^2-(Ex_i )^2 )+(Ex_i )^2 (Ew_i^2-(Ew_i )^2 )+(Ex_i^2-(Ex_i )^2 )(Ew_i^2-(Ew_i )^2 )
    $$
    
    $$
    =(Ew_i )^2 Ex_i^2-(Ew_i )^2 (Ex_i )^2+(Ex_i )^2 Ew_i^2-(Ew_i )^2 (Ex_i )^2+Ew_i^2 Ex_i^2
         -(Ew_i )^2 Ex_i^2-Ew_i^2 (Ex_i )^2+(Ew_i )^2 (Ex_i )^2
    $$
    
    $$
    =Ew_i^2 Ex_i^2-(Ew_i )^2 (Ex_i )^2
    $$
    
    משילוב שתי התוצאות קיבלנו את התוצאה הרצויה:
    
    $$
    Var(w_i x_i )=(Ew_i )^2 Var(x_i )+(Ex_i )^2 Var(w_i )+Var(x_i )Var(w_i )
    $$

3.
    תחת ההנחה כי התוחלות של המשקולות ושל הכניסה הן 0, הנוסחה שהוכחנו בסעיף הקודם מצטמצמת ל-
    $$
    Var(w_i x_i )=Var(x_i )Var(w_i )
    $$
    
    בנוסף, עבור מוצא הניורון v ניתן לכתוב:
    $$
    v=w_1 x_1+w_2 x_2+⋯+w_n x_n
    $$
    
    מכיוון שהנחנו שכל המשתנים מפולגים IID נקבל:
    $$
    Var(v)=nVar(w_1 x_1 )=nVar(w_1 )Var(x_1 )
    $$

4. 
    קיבלנו בסעיף הקודם כי שונות המוצא הוא שונות הכניסה מוכפל בפקטור של 
    $$nVar(w_i )$$ כאשר $$n$$ הוא מספר הניורונים המוזנים לניורון. לכן, על מנת לשמור על שונות זהה בין כניסה למוצא נבחר את אתחול המשקולות כך שיתקיים:
    
    $$
    Var(w_i )= \frac{1}{n}
    $$

**ב.**
1.
    אם התפלגות המשקולות סימטרית סביב 0, נקבל כי התפלגות $$y_i$$ סימטרית סביב 0 לכל $$i$$. לכן, ההנחה עדיין סבירה עבור tanh אך לא עבור ReLU וsigmoid, עבור tanh נקבל:
    
    $$
    Ex=E[\sigma(y)]=\int_{-\infty}^{-\infty} \sigma(y) f_Y (y)dy=0
    $$
    
    כאשר סימנו $$x=x_i,y=y_i$$.

2.
    משימוש בתוצאות הסעיפים הקודמים, נקבל:
    
    $$
    Var(v)=nVar(w_1 x_1 )=n((Ew_1 )^2 Var(x_1 )+(Ex_1 )^2 Var(w_1 )+Var(x_1 )Var(w_1 ))
    $$
    
    $$
    =nVar(w_1 )((Ex_1 )^2+Var(x_1 ))=nVar(w_1 )Ex_1^2
    $$

3. 
    אם התפלגות המשקולות סימטרית סביב 0, נקבל כי התפלגות $$y_i$$ סימטרית סביב 0 לכל $$i$$. לכן, עבור פונקציית אקטיבציה ReLU נקבל:
    
    $$
    Ex_1^2=E((max ((0,y_1 ) )^2 ) = \int_{-\infty}^{\infty} (max (0,y_1 ) )^2 f_Y (y)dy
    $$
    
    $$
    =\int_{0}^{\infty} y_1^2 f_Y (y)dy = \frac{1}{2} \int_{-\infty}^{\infty} y_1^2 f_Y (y)dy = 1/2 Ey_1^2= \frac{1}{2} Var(y_1 )
    $$
    
    נשלב את התוצאה יחד עם תוצאות הסעיפים הקודמים ונקבל:
    
    $$
    Var(v)=nVar(w_1 )Ex_1^2= \frac{n}{2} Var(w_1 )Var(y_1 )
    $$
    
    לכן, על מנת לשמור על שונות זהה בין כניסה למוצא נבחר את אתחול המשקולות כך שיתקיים:
    
    $$
    Var(w_i )= \frac{2}{n}
    $$

## חלק מעשי 

### LeNet-5
בחלק זה נעבור על היישום המעשי הראשון של רשתות קונבולוציה. הארכיטקטורה זאת שימשה ב1998 ושימש לזהות ספרות בכתב יד על צק'ים במערכות בנקאיות.
![Alt text](figs/lenet.gif)

הרשת מקבלת תמונה רמת אפור בגודל 32x32 ומשתמש באריטקטורה הבאה על מנת להוציא וקטור פלט באורך 10 אשר מציג את הסבירות שהתמונה שייכת לכל אחת מ 10 הספרות.  

###  אריכטקטורה 
![png](figs/lenet_arch.png)

אם לא צויין אחרת הארכיטקטורה לא עושה שימוש בריפוד ו-dilation. 

- C1: Convolutional layer + ReLU activation: kernel size=5x5, output channels=6.
- S2: Max pooling layer: size=2x2, stride=2
- C3: Convolutional layer + ReLU activation: kernel size=5x5, output channels=16.
- S4: Max pooling layer: size=2x2, stride=2
- C5: Convolutional layer + ReLU activation: kernel size=5x5, output channels=120. (this is, in fact, a fully connected layer)
- F6: Fully connected layer + ReLU: output vector length= 84
- Output layer: Fully connected layer: output vector length=10

על מנת לייצג הסתברות שהתמונה שייכת לאחת מהמחלקות נעשה ביציאה שימוש בשכבת Softmax, שלא נרחיב עליה בקורס.

### Dataset: MNIST
.לאימון הרשת נעשה שימוש במאגר המידע MNIST. הוא סט פופולרי מאוד שנעשה בו שימוש נרחב עד היום. הסט מורכב 70000  תמונות בינאריות בגודל 28x28 של ספרות בכתב יד, מתוכן 10000 הינם בסט המבחן.


ניתן להוריד את הסט מ
[Yann LeCun's web site](http://yann.lecun.com/exdb/mnist/) 
 או לחילופין, ישרות מ -PyTorch
 [torchvision.datasets.MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist)
 
### דוגמא מתוך המאגר 


Number of rows in the train dataset: $$N=60000$$

Number of rows in the test dataset: $$N=10000$$

Image size: 28x28

Pixels value range: [0.0, 1.0]

![png](output_21_4.png)


## 📜 הגדרת הבעיה 

- המשתנים בבעיה:
  - תמונה בגודל 28x28 של סיפרה בכתב יד - $$x$$
  - ערך הסיפרה: \[0-9\] - $$y$$

המודל יהיה החזאי $$\hat{y}=h^*\left(\boldsymbol{x}\right)$$ שתפקידו למזעזר את פונקציית המחיר misclassification rate:

$$
h^*=\underset{h}{\arg\min}\ E\left[I\left\lbrace h\left(\boldsymbol{x}\right)\neq y\right\rbrace\right]
$$

### 📚 חלוקה של מאגר המידע
משום שהמאגר המידע מחולק כבר לסט מבחן, כל מה שנותר לנו הוא לחלק את סט האימון לסט ולידציה וסט אימון.
אין צורך לקחת סט ולידציה גדול משום שהרצת המודל על סט גדול דורשת הרבה משאבים, נקח סט קטן בגודל 1024 על מנת להעריך את ביצועי המודל בתהליך הלמידה באופן מהיר וחסכוני.

## 💡 אלגוריתם למידה 

נשתמש באלגוריתם הגרדיאנט בגרסת ה-mini-batch, או בשמו stochastic gradient descent (SGD), המשמעות הסטוכנסטית היינה שהBatch מוגרל באופן אחיד מתך מאגר המידע.
האלגוריתם ישמש למציאת משקולות הרשת להקטנת פונקצית המחיר.

תזכורות: משקולות הרשת הינם המשקולות בגרעיני קונבולוציה, בשכבות FC ואיברי ההסט.


### Hyper-parameters

- הארכיטקטורה של הרשת, שבה לא נעשה כל שינוי.
- אלגוריתם SDG:
  - גודל צעד הלימוד.
  - גודל הBatch, אשר אותו נשאיר קבוע בגודל 64.
  - מס' מקסימלי של epochs. 
    epochs - מס המעברים על כל סט האימון


## ⚙️ Learning

### Selecting the Learning Rate

נבחן את השפעת גדלי צעד לימוד על תהליך הלימוד.
נריץ את האלגוריתם למשך epoch אחד עם הגדלים הבאים: $$\alpha=10^{0},10^{-1},10^{-2}$$
![png](output_34_0.png)

קיבלנו שעבור $$\alpha=10^{0}$$ המערכת לא מצליחה לבצע למידה (להקטין את פונקציית המחיר). ניתן להגיד שצעד הלימוד גדול מידי משום שעבור צעד לימוד קטן יותר המערכת כן מצליחה ללמוד.

כמו כן עבור $$\alpha=10^{-1}$$ קיבלנו תהליך התכנסות מהיר הרבה יותר ולמחיר נמוך יותר על פני $$10^{-2}$$. לכן נבחר את גודל הצעד להיות $$\alpha=10^{-1}$$   

## Training
נריץ את באלגוריתם עבור גודל צעד $$\alpha=10^{-1}$$ למשך 20 epochs.
![png](output_37_0.png)

נראה שמודל התכנס יפה, משום שמחיר על סט הולידציה הגיע למישור. ניתן להניח שהמודל הגיע למינימום מקומי (אבל לא ניתן לדעת בוודאות)


## ⏱️ הערכת ביצועים#
נריץ את המודל לאחר הלימוד על סט המבחן ונקבל שפונקצית המחיר היינה

The test risk is: $$0.0091$$

קיבלנו  misclassification rate של כמעט אחוז בודד, כלומר שעבור אחוז מסט המבחן טעינו בחיזוי הסיפרה או לחילופין צדקנו ב99% מסט המבחן. 
![png](output_42_0.png)
